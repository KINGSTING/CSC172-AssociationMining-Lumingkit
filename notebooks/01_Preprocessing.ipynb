{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5647ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset from https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain%2B_20Percent.txt...\n",
      "✅ Success! Dataset saved to: ../data/KDDTrain+_20Percent.txt\n",
      "Loading dataset...\n",
      "Initial shape: (25192, 42)\n",
      "\n",
      "Sample Processed Transaction:\n",
      "  Protocol   Service Flag    Class Duration Src_Bytes Dst_Bytes Traffic_Count\n",
      "0      tcp  ftp_data   SF   normal     Zero       Low      Zero        Normal\n",
      "1      udp     other   SF   normal     Zero       Low      Zero          High\n",
      "2      tcp   private   S0  neptune     Zero      Zero      Zero         Surge\n",
      "3      tcp      http   SF   normal     Zero       Low    Medium        Normal\n",
      "4      tcp      http   SF   normal     Zero       Low       Low          High\n",
      "\n",
      "Processed data saved to: ../data/processed_data.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Define the URL and the target path\n",
    "url = \"https://raw.githubusercontent.com/defcom17/NSL_KDD/master/KDDTrain%2B_20Percent.txt\"\n",
    "save_path = \"../data/KDDTrain+_20Percent.txt\"\n",
    "\n",
    "# Create the data folder if it doesn't exist (just in case)\n",
    "if not os.path.exists(\"../data\"):\n",
    "    os.makedirs(\"../data\")\n",
    "\n",
    "# Download the file\n",
    "print(f\"Downloading dataset from {url}...\")\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    print(f\"✅ Success! Dataset saved to: {save_path}\")\n",
    "else:\n",
    "    print(f\"❌ Error downloading file. Status code: {response.status_code}\")\n",
    "\n",
    "# NSL-KDD does not have headers in the raw file. We must define them manually.\n",
    "COL_NAMES = [\n",
    "    \"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
    "    \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\",\n",
    "    \"num_failed_logins\", \"logged_in\", \"num_compromised\", \"root_shell\",\n",
    "    \"su_attempted\", \"num_root\", \"num_file_creations\", \"num_shells\",\n",
    "    \"num_access_files\", \"num_outbound_cmds\", \"is_host_login\",\n",
    "    \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
    "    \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
    "    \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\",\n",
    "    \"dst_host_srv_count\", \"dst_host_same_srv_rate\",\n",
    "    \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
    "    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\",\n",
    "    \"dst_host_srv_serror_rate\", \"dst_host_rerror_rate\",\n",
    "    \"dst_host_srv_rerror_rate\", \"class\", \"difficulty_level\"\n",
    "]\n",
    "\n",
    "def load_and_process_data():\n",
    "    print(\"Loading dataset...\")\n",
    "    # Load data (CSV format, no header)\n",
    "    df = pd.read_csv(DATA_PATH, names=COL_NAMES, index_col=False)\n",
    "    \n",
    "    # Drop 'difficulty_level' (last column, not needed for intrusion detection)\n",
    "    df.drop('difficulty_level', axis=1, inplace=True)\n",
    "    \n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "    \n",
    "    # --- Step 1: Feature Selection ---\n",
    "    # For Association Rule Mining, we focus on the \"intrinsic\" attributes \n",
    "    # that define a connection, rather than the \"content\" (payload) metrics \n",
    "    # which can be infinite. We select the most relevant columns.\n",
    "    selected_features = [\n",
    "        'protocol_type', 'service', 'flag', 'class',  # Categorical (Keep as is)\n",
    "        'duration', 'src_bytes', 'dst_bytes', 'count' # Numerical (Need Binning)\n",
    "    ]\n",
    "    df = df[selected_features]\n",
    "\n",
    "    # --- Step 2: Binning Continuous Variables ---\n",
    "    # Apriori cannot handle \"src_bytes = 491\". It needs \"src_bytes = Low\".\n",
    "    # We use Quantile-based discretization (qcut) or fixed logic.\n",
    "    \n",
    "    # Binning Duration\n",
    "    # Most connections are 0 seconds. We create a 'Zero' bin and others.\n",
    "    df['duration_bin'] = pd.cut(df['duration'], \n",
    "                                bins=[-1, 0, 60, float('inf')], \n",
    "                                labels=['Zero', 'Short', 'Long'])\n",
    "    \n",
    "    # Binning Source Bytes (Traffic Volume)\n",
    "    # 0 = No data sent. \n",
    "    df['src_bytes_bin'] = pd.cut(df['src_bytes'], \n",
    "                                 bins=[-1, 0, 1000, 10000, float('inf')], \n",
    "                                 labels=['Zero', 'Low', 'Medium', 'High'])\n",
    "    \n",
    "    # Binning Destination Bytes\n",
    "    df['dst_bytes_bin'] = pd.cut(df['dst_bytes'], \n",
    "                                 bins=[-1, 0, 1000, 10000, float('inf')], \n",
    "                                 labels=['Zero', 'Low', 'Medium', 'High'])\n",
    "    \n",
    "    # Binning Count (Traffic spikes in past 2 seconds)\n",
    "    df['count_bin'] = pd.cut(df['count'], \n",
    "                             bins=[-1, 10, 100, float('inf')], \n",
    "                             labels=['Normal', 'High', 'Surge'])\n",
    "\n",
    "    # --- Step 3: Final Formatting ---\n",
    "    # Drop the original number columns, keep only bins + categorical\n",
    "    final_cols = ['protocol_type', 'service', 'flag', 'class', \n",
    "                  'duration_bin', 'src_bytes_bin', 'dst_bytes_bin', 'count_bin']\n",
    "    \n",
    "    df_clean = df[final_cols].copy()\n",
    "    \n",
    "    # Rename columns to make rules readable (e.g. \"duration_bin\" -> \"Duration\")\n",
    "    df_clean.columns = ['Protocol', 'Service', 'Flag', 'Class', \n",
    "                        'Duration', 'Src_Bytes', 'Dst_Bytes', 'Traffic_Count']\n",
    "\n",
    "    # Convert all to string to ensure they are treated as categorical items\n",
    "    df_clean = df_clean.astype(str)\n",
    "    \n",
    "    print(\"\\nSample Processed Transaction:\")\n",
    "    print(df_clean.head())\n",
    "    \n",
    "    # Save to CSV\n",
    "    if not os.path.exists('../data'):\n",
    "        os.makedirs('../data')\n",
    "    \n",
    "    df_clean.to_csv(OUTPUT_PATH, index=False)\n",
    "    print(f\"\\nProcessed data saved to: {OUTPUT_PATH}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    load_and_process_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
